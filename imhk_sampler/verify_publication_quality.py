#!/usr/bin/env python3
"""
verify_publication_quality.py - Verification of Publication Quality Results

This script validates that the results generated by the publication_results.py script
meet the standards required for research publication. It performs comprehensive checks on:
1. Statistical significance of the results
2. Quality of visualizations
3. Key research findings and their validity
4. Overall assessment of publication readiness

The script analyzes the contents of the results directory and provides a detailed report
on whether the results are publication-ready with clear pass/fail indicators.

Usage:
    python verify_publication_quality.py [--detailed] [--results-dir PATH]

Options:
    --detailed      Show detailed analysis for each check
    --results-dir   Path to results directory (default: ./results/publication)

Author: Quantum MCMC Research Team
"""

import os
import sys
import json
import pickle
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional, Union
import math
import glob
import re

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("verify_publication.log", mode='w')
    ]
)
logger = logging.getLogger("publication_verification")

# Constants and thresholds for verification
PUBLICATION_CONSTANTS = {
    # Statistical thresholds
    "MIN_SAMPLE_SIZE": 5000,              # Minimum sample size for publication
    "MIN_ACCEPTANCE_RATE": 0.2,           # Minimum acceptance rate for valid MCMC
    "OPTIMAL_ACCEPTANCE_RATE": 0.234,     # Optimal acceptance rate for MH algorithms
    "MIN_ESS_RATIO": 0.1,                 # Minimum ratio of ESS to sample size
    "MIN_ESS_FOR_PUBLICATION": 1000,      # Minimum effective sample size for publication
    
    # Visualization thresholds
    "MIN_DPI": 300,                       # Minimum DPI for publication-quality images
    "MIN_PLOT_SIZE": (8, 6),              # Minimum plot dimensions (inches)
    "REQUIRED_PLOT_TYPES": [              # Required visualization types
        "baseline_comparison",            # Baseline comparison between IMHK and Klein
        "acceptance_vs_sigma",            # Acceptance rate vs sigma
        "tv_distance",                    # Total variation distance plots
        "tv_ratio",                       # TV ratio comparisons
        "quality_vs_speed",               # Quality vs. speed trade-off
        "algorithm_selection"             # Guidance plot for algorithm selection
    ],
    
    # Research findings thresholds
    "MIN_IMPROVEMENT_PCT": 10,            # Minimum percentage improvement for significance
    "SIGNIFICANT_IMPROVEMENT_PCT": 25,    # Percentage for "significant" improvement claim
    "MAX_TV_DISTANCE": 0.05,              # Maximum TV distance for high-quality samples
    
    # File structure expectations
    "EXPECTED_EXPERIMENT_DIRS": [         # Expected experiment directories
        "baseline",
        "ill_conditioned",
        "parameter_sweep",
        "convergence",
        "summary"
    ]
}

class VerificationResult:
    """Class to track verification results with pass/fail status and detailed messages."""
    
    def __init__(self, category: str):
        self.category = category
        self.passed = True
        self.messages = []
        self.details = []
    
    def fail(self, message: str, detail: Optional[str] = None):
        """Record a failed check."""
        self.passed = False
        self.messages.append(f"✗ FAIL: {message}")
        if detail:
            self.details.append(detail)
    
    def warn(self, message: str, detail: Optional[str] = None):
        """Record a warning (doesn't fail the check)."""
        self.messages.append(f"⚠ WARNING: {message}")
        if detail:
            self.details.append(detail)
    
    def pass_check(self, message: str, detail: Optional[str] = None):
        """Record a passed check."""
        self.messages.append(f"✓ PASS: {message}")
        if detail:
            self.details.append(detail)
    
    def print_summary(self, detailed: bool = False):
        """Print a summary of the verification results."""
        status = "PASSED" if self.passed else "FAILED"
        print(f"\n{'-'*80}")
        print(f"{self.category} Verification: {status}")
        print(f"{'-'*80}")
        
        for msg in self.messages:
            print(msg)
        
        if detailed and self.details:
            print("\nDetails:")
            for detail in self.details:
                print(f"  {detail}")
                
        return self.passed


class PublicationVerifier:
    """
    Class to verify that research results meet publication standards.
    
    This verifier analyzes the contents of the results directory and checks:
    1. Statistical significance of the results
    2. Quality of visualizations
    3. Key research findings and their validity
    """
    
    def __init__(self, results_dir: str = "results/publication"):
        """
        Initialize the verifier with paths to result directories.
        
        Args:
            results_dir: Path to the main results directory
        """
        self.results_dir = Path(results_dir)
        
        # Check if the directory exists
        if not self.results_dir.exists():
            logger.error(f"Results directory not found: {self.results_dir}")
            raise FileNotFoundError(f"Results directory not found: {self.results_dir}")
        
        # Initialize paths to important subdirectories and files
        self.baseline_dir = self.results_dir / "baseline"
        self.ill_conditioned_dir = self.results_dir / "ill_conditioned"
        self.parameter_sweep_dir = self.results_dir / "parameter_sweep"
        self.convergence_dir = self.results_dir / "convergence"
        self.summary_dir = self.results_dir / "summary"
        
        # Initialize results
        self.results = {}
        
        # Store the summary report for later analysis
        self.summary_report = None
        
        # Dictionary to store all verification results
        self.verification_results = {}
    
    def load_summary_report(self) -> Dict[str, Any]:
        """
        Load the research summary report.
        
        Returns:
            Dict containing the research summary
        """
        summary_file = self.summary_dir / "research_summary.json"
        if not summary_file.exists():
            logger.error(f"Summary report not found: {summary_file}")
            return {}
        
        try:
            with open(summary_file, 'r') as f:
                summary = json.load(f)
            logger.info(f"Loaded summary report from {summary_file}")
            self.summary_report = summary
            return summary
        except Exception as e:
            logger.error(f"Error loading summary report: {e}")
            return {}
    
    def load_experiment_results(self, experiment: str) -> Dict[str, Any]:
        """
        Load results for a specific experiment.
        
        Args:
            experiment: Name of the experiment ('baseline', 'ill_conditioned', etc.)
            
        Returns:
            Dict containing the experiment results
        """
        experiment_dir = self.results_dir / experiment / "data"
        if not experiment_dir.exists():
            logger.warning(f"Experiment directory not found: {experiment_dir}")
            return {}
        
        # Find pickle files in the directory
        pickle_files = list(experiment_dir.glob("*.pickle"))
        if not pickle_files:
            logger.warning(f"No pickle files found in {experiment_dir}")
            return {}
        
        # Load the first pickle file found (main results file)
        try:
            with open(pickle_files[0], 'rb') as f:
                results = pickle.load(f)
            logger.info(f"Loaded results from {pickle_files[0]}")
            self.results[experiment] = results
            return results
        except Exception as e:
            logger.error(f"Error loading results for {experiment}: {e}")
            return {}
    
    def verify_directory_structure(self) -> VerificationResult:
        """
        Verify that the directory structure meets expectations.
        
        Returns:
            VerificationResult with pass/fail status
        """
        result = VerificationResult("Directory Structure")
        
        # Check that all expected directories exist
        for dir_name in PUBLICATION_CONSTANTS["EXPECTED_EXPERIMENT_DIRS"]:
            dir_path = self.results_dir / dir_name
            if not dir_path.exists():
                result.fail(f"Missing required directory: {dir_name}")
            else:
                # Check that data and plots subdirectories exist
                data_dir = dir_path / "data"
                plots_dir = dir_path / "plots"
                
                if not data_dir.exists():
                    result.fail(f"Missing data directory for {dir_name}")
                elif not list(data_dir.glob("*")):
                    result.warn(f"No data files found in {dir_name}/data")
                
                if not plots_dir.exists():
                    result.fail(f"Missing plots directory for {dir_name}")
                elif not list(plots_dir.glob("*")):
                    result.warn(f"No plot files found in {dir_name}/plots")
        
        # Overall check
        if result.passed:
            result.pass_check("Directory structure meets publication requirements")
            result.pass_check(f"Found all required experiment directories")
        
        self.verification_results["directory_structure"] = result
        return result
    
    def check_visualization_quality(self) -> VerificationResult:
        """
        Check the quality of visualizations to ensure they meet publication standards.
        
        Returns:
            VerificationResult with pass/fail status
        """
        result = VerificationResult("Visualization Quality")
        
        # Find all plot files
        all_plots = []
        for exp_dir in PUBLICATION_CONSTANTS["EXPECTED_EXPERIMENT_DIRS"]:
            plot_dir = self.results_dir / exp_dir / "plots"
            if plot_dir.exists():
                all_plots.extend(list(plot_dir.glob("*.png")))
        
        if not all_plots:
            result.fail("No visualization plots found")
            self.verification_results["visualization_quality"] = result
            return result
        
        # Keep track of plot types we find
        plot_types_found = set()
        
        # Analyze each plot
        plots_analyzed = 0
        plots_with_issues = 0
        
        for plot_file in all_plots:
            try:
                # Extract plot type from filename
                plot_name = plot_file.name
                for plot_type in PUBLICATION_CONSTANTS["REQUIRED_PLOT_TYPES"]:
                    if plot_type in plot_name.lower():
                        plot_types_found.add(plot_type)
                
                # Open image and check properties
                with Image.open(plot_file) as img:
                    width, height = img.size
                    dpi = self._extract_dpi_from_image(img)
                    
                    # Check DPI
                    if dpi is None or dpi < PUBLICATION_CONSTANTS["MIN_DPI"]:
                        plots_with_issues += 1
                        result.warn(f"Plot has low DPI: {plot_name}", 
                                   f"DPI: {dpi}, recommended: {PUBLICATION_CONSTANTS['MIN_DPI']}")
                    
                    # Check dimensions (approximating inches based on DPI)
                    actual_dpi = dpi if dpi is not None else PUBLICATION_CONSTANTS["MIN_DPI"]
                    width_inches = width / actual_dpi
                    height_inches = height / actual_dpi
                    
                    min_width, min_height = PUBLICATION_CONSTANTS["MIN_PLOT_SIZE"]
                    
                    if width_inches < min_width or height_inches < min_height:
                        plots_with_issues += 1
                        result.warn(f"Plot has small dimensions: {plot_name}", 
                                   f"Size: {width_inches:.1f}x{height_inches:.1f} inches, " +
                                   f"recommended: {min_width}x{min_height} inches")
                
                plots_analyzed += 1
                
            except Exception as e:
                logger.error(f"Error analyzing plot {plot_file}: {e}")
                plots_with_issues += 1
        
        # Check if all required plot types are present
        for required_type in PUBLICATION_CONSTANTS["REQUIRED_PLOT_TYPES"]:
            if required_type not in plot_types_found:
                result.fail(f"Missing required plot type: {required_type}")
        
        # Overall assessment
        if plots_analyzed == 0:
            result.fail("No plots could be analyzed")
        elif plots_with_issues / plots_analyzed > 0.25:  # More than 25% of plots have issues
            result.fail(f"{plots_with_issues} out of {plots_analyzed} plots have quality issues")
        else:
            result.pass_check(f"Analyzed {plots_analyzed} plots, {plots_with_issues} have minor issues")
            
            if len(plot_types_found) >= len(PUBLICATION_CONSTANTS["REQUIRED_PLOT_TYPES"]):
                result.pass_check(f"Found all required plot types")
            else:
                result.pass_check(f"Found {len(plot_types_found)} of {len(PUBLICATION_CONSTANTS['REQUIRED_PLOT_TYPES'])} required plot types")
        
        self.verification_results["visualization_quality"] = result
        return result
    
    def _extract_dpi_from_image(self, img: Image.Image) -> Optional[float]:
        """
        Extract DPI information from an image.
        
        Args:
            img: PIL Image object
            
        Returns:
            DPI as a float, or None if not available
        """
        try:
            dpi = img.info.get('dpi')
            if dpi and len(dpi) >= 2:
                # Take the average of x and y dpi
                return (dpi[0] + dpi[1]) / 2
            return None
        except:
            return None
    
    def verify_statistical_significance(self) -> VerificationResult:
        """
        Verify the statistical significance of the results.
        
        Checks:
        - Sample sizes are sufficient for publication
        - Effective Sample Size (ESS) is adequate
        - Acceptance rates are within acceptable range
        
        Returns:
            VerificationResult with pass/fail status
        """
        result = VerificationResult("Statistical Significance")
        
        # Make sure we have loaded the necessary results
        if not self.results:
            # Try to load the main experiments
            for exp in ["baseline", "ill_conditioned", "convergence"]:
                self.load_experiment_results(exp)
        
        if not self.results:
            result.fail("No experiment results could be loaded")
            self.verification_results["statistical_significance"] = result
            return result
        
        # Check sample sizes
        for exp_name, exp_data in self.results.items():
            # Handle different experiment data structures
            if exp_name == "baseline":
                self._check_sample_size_baseline(exp_data, result)
            elif exp_name == "ill_conditioned":
                self._check_sample_size_ill_conditioned(exp_data, result)
            elif exp_name == "convergence":
                self._check_sample_size_convergence(exp_data, result)
        
        # Check acceptance rates
        for exp_name, exp_data in self.results.items():
            # Again, handle different data structures
            if exp_name == "baseline":
                self._check_acceptance_rate_baseline(exp_data, result)
            elif exp_name == "ill_conditioned":
                self._check_acceptance_rate_ill_conditioned(exp_data, result)
            elif exp_name == "convergence":
                self._check_acceptance_rate_convergence(exp_data, result)
        
        # Check effective sample sizes (ESS)
        for exp_name, exp_data in self.results.items():
            if exp_name == "convergence":
                self._check_ess_convergence(exp_data, result)
        
        # Overall assessment
        if result.passed:
            result.pass_check("All statistical checks passed")
        
        self.verification_results["statistical_significance"] = result
        return result
    
    def _check_sample_size_baseline(self, data: Dict[str, Any], result: VerificationResult):
        """Check sample size for baseline experiment."""
        if 'num_samples' in data:
            num_samples = data['num_samples']
            if num_samples < PUBLICATION_CONSTANTS["MIN_SAMPLE_SIZE"]:
                result.fail(f"Baseline experiment has insufficient sample size: {num_samples}",
                          f"Required: {PUBLICATION_CONSTANTS['MIN_SAMPLE_SIZE']}")
            else:
                result.pass_check(f"Baseline experiment has adequate sample size: {num_samples}")
        else:
            result.warn("Could not find sample size information for baseline experiment")
    
    def _check_sample_size_ill_conditioned(self, data: Dict[str, Any], result: VerificationResult):
        """Check sample size for ill-conditioned experiment."""
        # For ill-conditioned, data is a dictionary with sigma values as keys
        if isinstance(data, dict) and data:
            # Check the first result
            first_sigma = next(iter(data))
            if 'num_samples' in data[first_sigma]:
                num_samples = data[first_sigma]['num_samples']
                if num_samples < PUBLICATION_CONSTANTS["MIN_SAMPLE_SIZE"]:
                    result.fail(f"Ill-conditioned experiment has insufficient sample size: {num_samples}",
                            f"Required: {PUBLICATION_CONSTANTS['MIN_SAMPLE_SIZE']}")
                else:
                    result.pass_check(f"Ill-conditioned experiment has adequate sample size: {num_samples}")
            else:
                result.warn("Could not find sample size information for ill-conditioned experiment")
        else:
            result.warn("Could not analyze ill-conditioned experiment data")
    
    def _check_sample_size_convergence(self, data: Dict[str, Any], result: VerificationResult):
        """Check sample size for convergence experiment."""
        for config, config_data in data.items():
            if isinstance(config_data, dict) and 'error' not in config_data:
                # Only check configurations without errors
                if any(key.startswith('imhk_') for key in config_data.keys()):
                    # This looks like a valid configuration
                    sample_size_keys = ['num_samples', 'samples']
                    for key in sample_size_keys:
                        if key in config_data:
                            num_samples = config_data[key]
                            if isinstance(num_samples, (int, float)) and num_samples < PUBLICATION_CONSTANTS["MIN_SAMPLE_SIZE"]:
                                result.fail(f"Configuration {config} has insufficient sample size: {num_samples}",
                                        f"Required: {PUBLICATION_CONSTANTS['MIN_SAMPLE_SIZE']}")
                            else:
                                result.pass_check(f"Configuration {config} has adequate sample size")
                            break
                    else:
                        result.warn(f"Could not find sample size information for configuration {config}")
    
    def _check_acceptance_rate_baseline(self, data: Dict[str, Any], result: VerificationResult):
        """Check acceptance rate for baseline experiment."""
        if 'imhk_acceptance_rate' in data:
            rate = data['imhk_acceptance_rate']
            self._assess_acceptance_rate(rate, "Baseline experiment", result)
        else:
            result.warn("Could not find acceptance rate for baseline experiment")
    
    def _check_acceptance_rate_ill_conditioned(self, data: Dict[str, Any], result: VerificationResult):
        """Check acceptance rate for ill-conditioned experiment."""
        if isinstance(data, dict) and data:
            # Check all sigma values
            for sigma, sigma_data in data.items():
                if isinstance(sigma_data, dict) and 'imhk_acceptance_rate' in sigma_data:
                    rate = sigma_data['imhk_acceptance_rate']
                    self._assess_acceptance_rate(rate, f"Ill-conditioned (σ={sigma})", result)
                else:
                    result.warn(f"Could not find acceptance rate for ill-conditioned experiment (σ={sigma})")
        else:
            result.warn("Could not analyze ill-conditioned experiment data")
    
    def _check_acceptance_rate_convergence(self, data: Dict[str, Any], result: VerificationResult):
        """Check acceptance rate for convergence experiment."""
        for config, config_data in data.items():
            if isinstance(config_data, dict) and 'error' not in config_data:
                if 'imhk_acceptance_rate' in config_data:
                    rate = config_data['imhk_acceptance_rate']
                    self._assess_acceptance_rate(rate, f"Configuration {config}", result)
                else:
                    result.warn(f"Could not find acceptance rate for configuration {config}")
    
    def _assess_acceptance_rate(self, rate: float, context: str, result: VerificationResult):
        """Assess whether an acceptance rate is within acceptable range."""
        if rate < PUBLICATION_CONSTANTS["MIN_ACCEPTANCE_RATE"]:
            result.fail(f"{context} has low acceptance rate: {rate:.3f}",
                      f"Minimum recommended: {PUBLICATION_CONSTANTS['MIN_ACCEPTANCE_RATE']}")
        elif abs(rate - PUBLICATION_CONSTANTS["OPTIMAL_ACCEPTANCE_RATE"]) < 0.05:
            # Within 0.05 of optimal rate (around the theoretical optimal for MH)
            result.pass_check(f"{context} has near-optimal acceptance rate: {rate:.3f}")
        else:
            result.pass_check(f"{context} has acceptable acceptance rate: {rate:.3f}")
    
    def _check_ess_convergence(self, data: Dict[str, Any], result: VerificationResult):
        """Check effective sample size for convergence experiments."""
        for config, config_data in data.items():
            if isinstance(config_data, dict) and 'error' not in config_data:
                if 'imhk_ess' in config_data:
                    ess_values = config_data['imhk_ess']
                    if isinstance(ess_values, list) and ess_values:
                        # Calculate mean ESS
                        mean_ess = sum(ess_values) / len(ess_values)
                        
                        # Get sample size if available
                        sample_size = config_data.get('num_samples', 0)
                        if sample_size > 0:
                            ess_ratio = mean_ess / sample_size
                            
                            if mean_ess < PUBLICATION_CONSTANTS["MIN_ESS_FOR_PUBLICATION"]:
                                result.fail(f"{config} has low effective sample size: {mean_ess:.1f}",
                                          f"Minimum recommended: {PUBLICATION_CONSTANTS['MIN_ESS_FOR_PUBLICATION']}")
                            elif ess_ratio < PUBLICATION_CONSTANTS["MIN_ESS_RATIO"]:
                                result.warn(f"{config} has low ESS ratio: {ess_ratio:.3f}",
                                         f"ESS: {mean_ess:.1f} / Sample size: {sample_size}")
                            else:
                                result.pass_check(f"{config} has good effective sample size: {mean_ess:.1f} (ratio: {ess_ratio:.3f})")
                        else:
                            # Just check absolute ESS value
                            if mean_ess < PUBLICATION_CONSTANTS["MIN_ESS_FOR_PUBLICATION"]:
                                result.fail(f"{config} has low effective sample size: {mean_ess:.1f}")
                            else:
                                result.pass_check(f"{config} has good effective sample size: {mean_ess:.1f}")
                else:
                    result.warn(f"Could not find ESS information for configuration {config}")
    
    def verify_research_findings(self) -> VerificationResult:
        """
        Verify the key research findings.
        
        Checks:
        - IMHK outperforms Klein for ill-conditioned bases
        - Impact of sigma parameter on performance
        - Improvement percentages across different basis types
        
        Returns:
            VerificationResult with pass/fail status
        """
        result = VerificationResult("Research Findings")
        
        # Load the summary report if not already loaded
        if self.summary_report is None:
            self.load_summary_report()
        
        if not self.summary_report:
            result.fail("Could not load summary report for research findings verification")
            self.verification_results["research_findings"] = result
            return result
        
        # Analyze key findings
        if 'key_findings' in self.summary_report:
            key_findings = self.summary_report['key_findings']
            
            # Check if we have enough key findings
            if len(key_findings) < 3:
                result.warn("Few key findings documented in the summary report",
                           f"Found {len(key_findings)} findings, expected at least 3")
            
            # Look for specific claims in the findings
            self._verify_imhk_outperformance(key_findings, result)
            self._verify_sigma_impact(key_findings, result)
        else:
            result.fail("No key findings found in the summary report")
        
        # Verify improvement percentages from the actual data
        self._verify_improvement_percentages(result)
        
        # Overall assessment
        if result.passed:
            result.pass_check("Research findings validation passed")
        
        self.verification_results["research_findings"] = result
        return result
    
    def _verify_imhk_outperformance(self, findings: List[str], result: VerificationResult):
        """Verify that findings indicate IMHK outperforms Klein for ill-conditioned bases."""
        outperformance_found = False
        improvement_percentage = None
        
        for finding in findings:
            # Look for statements about improvement for ill-conditioned lattices
            if "ill-conditioned" in finding.lower() and "improvement" in finding.lower():
                outperformance_found = True
                
                # Try to extract the percentage improvement
                percentage_match = re.search(r'(\d+(?:\.\d+)?)%', finding)
                if percentage_match:
                    improvement_percentage = float(percentage_match.group(1))
        
        if outperformance_found:
            result.pass_check("Key finding confirms IMHK outperforms Klein for ill-conditioned bases")
            
            if improvement_percentage is not None:
                if improvement_percentage >= PUBLICATION_CONSTANTS["SIGNIFICANT_IMPROVEMENT_PCT"]:
                    result.pass_check(f"IMHK shows significant improvement: {improvement_percentage}%",
                                    f"(Threshold for significance: {PUBLICATION_CONSTANTS['SIGNIFICANT_IMPROVEMENT_PCT']}%)")
                elif improvement_percentage < PUBLICATION_CONSTANTS["MIN_IMPROVEMENT_PCT"]:
                    result.warn(f"IMHK improvement percentage is low: {improvement_percentage}%",
                               f"(Minimum recommended: {PUBLICATION_CONSTANTS['MIN_IMPROVEMENT_PCT']}%)")
                else:
                    result.pass_check(f"IMHK shows moderate improvement: {improvement_percentage}%")
        else:
            result.fail("No clear finding about IMHK outperforming Klein for ill-conditioned bases")
    
    def _verify_sigma_impact(self, findings: List[str], result: VerificationResult):
        """Verify that findings indicate the impact of sigma parameter on performance."""
        sigma_impact_found = False
        
        for finding in findings:
            # Look for statements about sigma parameter impact
            if "sigma" in finding.lower() or "σ" in finding.lower():
                if "ratio" in finding.lower() or "impact" in finding.lower() or "affect" in finding.lower():
                    sigma_impact_found = True
        
        if sigma_impact_found:
            result.pass_check("Key findings include impact of sigma parameter on sampling performance")
        else:
            result.warn("No clear finding about the impact of sigma parameter on performance")
    
    def _verify_improvement_percentages(self, result: VerificationResult):
        """Verify improvement percentages across different basis types from actual data."""
        # Check summary data for most detailed analysis
        if self.summary_report and 'parameter_sweep_insights' in self.summary_report:
            insights = self.summary_report['parameter_sweep_insights']
            
            if 'best_performance_by_basis' in insights:
                basis_performance = insights['best_performance_by_basis']
                
                for basis_type, data in basis_performance.items():
                    if 'ratio' in data:
                        ratio = data['ratio']
                        # A ratio < 1 means IMHK is better (smaller TV distance)
                        if ratio < 1:
                            improvement_pct = (1 - ratio) * 100
                            
                            if improvement_pct >= PUBLICATION_CONSTANTS["SIGNIFICANT_IMPROVEMENT_PCT"]:
                                result.pass_check(f"IMHK shows significant improvement for {basis_type} basis: {improvement_pct:.1f}%")
                            elif improvement_pct < PUBLICATION_CONSTANTS["MIN_IMPROVEMENT_PCT"]:
                                result.warn(f"IMHK shows minimal improvement for {basis_type} basis: {improvement_pct:.1f}%")
                            else:
                                result.pass_check(f"IMHK shows moderate improvement for {basis_type} basis: {improvement_pct:.1f}%")
                        else:
                            # IMHK performs worse (higher ratio)
                            worse_pct = (ratio - 1) * 100
                            result.warn(f"Klein outperforms IMHK for {basis_type} basis by {worse_pct:.1f}%")
                    else:
                        result.warn(f"Could not determine improvement ratio for {basis_type} basis")
            else:
                result.warn("No detailed basis performance data found in summary report")
        
        # Also check ill-conditioned experiment directly if available
        if "ill_conditioned" in self.results:
            ill_cond_data = self.results["ill_conditioned"]
            
            best_improvement = 0
            best_sigma = None
            
            for sigma, data in ill_cond_data.items():
                if isinstance(data, dict) and 'imhk_tv_distance' in data and 'klein_tv_distance' in data:
                    imhk_tv = data['imhk_tv_distance']
                    klein_tv = data['klein_tv_distance']
                    
                    if klein_tv > 0:
                        ratio = imhk_tv / klein_tv
                        if ratio < 1:
                            improvement_pct = (1 - ratio) * 100
                            if improvement_pct > best_improvement:
                                best_improvement = improvement_pct
                                best_sigma = sigma
            
            if best_sigma is not None:
                if best_improvement >= PUBLICATION_CONSTANTS["SIGNIFICANT_IMPROVEMENT_PCT"]:
                    result.pass_check(f"Verified significant IMHK improvement for ill-conditioned basis: {best_improvement:.1f}% (σ={best_sigma})")
                elif best_improvement < PUBLICATION_CONSTANTS["MIN_IMPROVEMENT_PCT"]:
                    result.warn(f"Verified minimal IMHK improvement for ill-conditioned basis: {best_improvement:.1f}% (σ={best_sigma})")
                else:
                    result.pass_check(f"Verified moderate IMHK improvement for ill-conditioned basis: {best_improvement:.1f}% (σ={best_sigma})")
    
    def perform_overall_verification(self, detailed: bool = False) -> bool:
        """
        Perform all verification checks and provide an overall assessment.
        
        Args:
            detailed: Whether to show detailed analysis for each check
            
        Returns:
            bool: True if all checks passed, False otherwise
        """
        # Run all verification checks
        structure_check = self.verify_directory_structure()
        visualization_check = self.check_visualization_quality()
        statistical_check = self.verify_statistical_significance()
        research_check = self.verify_research_findings()
        
        # Print detailed results for each check
        structure_check.print_summary(detailed)
        visualization_check.print_summary(detailed)
        statistical_check.print_summary(detailed)
        research_check.print_summary(detailed)
        
        # Print overall assessment
        passed = (structure_check.passed and 
                 visualization_check.passed and 
                 statistical_check.passed and 
                 research_check.passed)
        
        print("\n" + "="*80)
        print(f"OVERALL PUBLICATION READINESS ASSESSMENT: {'PASSED' if passed else 'FAILED'}")
        print("="*80)
        
        if passed:
            print("\n✓ Results are PUBLICATION READY")
            print("\nAll verification checks have passed. The results meet the")
            print("quality standards required for research publication.")
        else:
            print("\n✗ Results are NOT YET PUBLICATION READY")
            print("\nSome verification checks have failed. Please address the")
            print("issues highlighted above before submitting for publication.")
        
        return passed

def main():
    """Main entry point for the script."""
    parser = argparse.ArgumentParser(
        description='Verify publication quality of IMHK sampler results'
    )
    parser.add_argument('--detailed', action='store_true',
                        help='Show detailed analysis for each check')
    parser.add_argument('--results-dir', type=str, default='results/publication',
                        help='Path to results directory')
    
    args = parser.parse_args()
    
    try:
        # Create and run the verifier
        verifier = PublicationVerifier(args.results_dir)
        passed = verifier.perform_overall_verification(args.detailed)
        
        # Use exit code for scripting
        sys.exit(0 if passed else 1)
        
    except Exception as e:
        logger.error(f"Error during verification: {e}", exc_info=True)
        print(f"\nError during verification: {e}")
        sys.exit(2)

if __name__ == "__main__":
    main()
